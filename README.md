# üê∏ peppe - Fine-tuning GPT-2 com Greentexts do 4chan

Este projeto realiza o fine-tuning do modelo GPT-2 utilizando um dataset de greentexts coletados do 4chan. 

## üìñ Sobre o Projeto

O **peppe** √© um experimento de aprendizado de m√°quina que treina o modelo de linguagem GPT-2 da OpenAI para gerar textos no estilo caracter√≠stico das "greentexts" ‚Äî hist√≥rias curtas e humor√≠sticas originadas nos f√≥runs do 4chan, tipicamente escritas em linhas que come√ßam com `>`.

## üéØ Objetivo

O objetivo principal √© fazer com que o modelo aprenda o estilo √∫nico de escrita das greentexts, incluindo:
- Formato de texto com linhas iniciando com `>`
- Narrativa em primeira pessoa
- Tom humor√≠stico e absurdo
- Estrutura t√≠pica de "hist√≥ria de an√¥nimo"

## üì¶ Depend√™ncias

O projeto utiliza as seguintes bibliotecas principais:

- **transformers** (>=4.57.3) - Para carregar e treinar o modelo GPT-2
- **torch** (>=2. 9.1) - Framework de deep learning
- **datasets** (>=4. 4.1) - Para manipula√ß√£o do dataset
- **tiktoken** (>=0.12.0) - Tokeniza√ß√£o
- **tqdm** (>=4.67.1) - Barras de progresso

## üöÄ Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone https://github. com/mnsgrosa/peppe.git
cd peppe

# Instale as depend√™ncias usando uv
uv sync

# Ou usando pip
pip install -e .
```

## üìÇ Estrutura do Projeto

```
peppe/
‚îú‚îÄ‚îÄ main.py              # Script principal
‚îú‚îÄ‚îÄ src/                 # C√≥digo fonte do projeto
‚îú‚îÄ‚îÄ greentext_data/      # Dataset de greentexts
‚îú‚îÄ‚îÄ log/                 # Logs de treinamento
‚îú‚îÄ‚îÄ pyproject.toml       # Configura√ß√µes do projeto
‚îî‚îÄ‚îÄ README.md            # Este arquivo
```

## üóÉÔ∏è Dataset

O dataset utilizado consiste em greentexts coletadas do 4chan.  Greentexts s√£o um formato de postagem caracter√≠stico dos imageboards, onde as linhas come√ßam com o s√≠mbolo `>` (que aparece em verde no site original, da√≠ o nome). 

### Caracter√≠sticas do Dataset:
- Formato de texto √∫nico e reconhec√≠vel
- Hist√≥rias curtas e narrativas
- Conte√∫do humor√≠stico e sat√≠rico
- Linguagem informal da internet

## üß† Sobre o GPT-2

O GPT-2 (Generative Pre-trained Transformer 2) √© um modelo de linguagem desenvolvido pela OpenAI. Atrav√©s do processo de fine-tuning, adaptamos o modelo pr√©-treinado para gerar textos espec√≠ficos no estilo greentext.

### Processo de Treinamento:
1. Carregamento do modelo GPT-2 pr√©-treinado
2. Prepara√ß√£o e tokeniza√ß√£o do dataset de greentexts
3. Fine-tuning do modelo com os dados espec√≠ficos
4.  Avalia√ß√£o e gera√ß√£o de novos textos

## üìù Uso

Para rodar o projeto e necessario criar o dataset dos greentexts do 4chan, para treinar o modelo.


1) Download do dataset

```python
uv run src/model/greentexts.py
```

2) Treinamento do modelo

```python
uv run src/model/train.py
```

3) Ataque ao modelo

O modelo por ser um gerador de texto podemos atacar gerando dados ofensivos ou preconceituosos, para isso basta passar um prompt simples que o modelo ira completar o texto.

```python
uv run src/model/attack.py
```

Isso ira gerar o txt com o prompt base em um txt

4) Caso queira testar o modelo por conta propria basta usar o seguinte trecho de codigo

```python
import torch
import tiktoken
from gpt2 import GPT, GPTConfig
from torch.nn import functional as F

model = GPT(
    GPTConfig(block_size=256, vocab_size=50304, n_layer=4, n_head=4, n_embd=256)
)

enc = tiktoken.get_encoding("gpt2")

weights_path = "./weights/gpt2_weights.pth"
model.load_state_dict(torch.load(weights_path))

enc_input = torch.tensor(enc.encode("Seu prompt aqui"), dtype=torch.long)
enc_input = enc_input.unsqueeze(0).repeat(4, 1)

model.eval()
with torch.no_grad():
    with torch.autocast(device_type = "cpu", dtype = torch.bfloat16):
        logits, loss = model(input)
    logits = logits[:, -1, :]
    probs = F.softmax(logits, dim=-1)
    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)
    generator = torch.Generator().manual_seed(42)
    ix = torch.multinomial(topk_probs, 1, generator=generator)
    xcol = torch.gather(topk_indices, -1, ix)
    text = torch.cat((text, xcol), dim=1)
for i in range(4):
    tokens = text[i, :32].tolist()
    decoded = enc.decode(tokens)

with open("output.txt", "a") as f:
    f.write(f"Output {i+1}:\n{decoded}\n\n")
```

## ‚ö†Ô∏è Aviso

Este projeto √© puramente educacional e experimental. O conte√∫do gerado pelo modelo pode refletir o estilo e tom do dataset de treinamento.  Use com responsabilidade. 

## üìÑ Licen√ßa

Este projeto √© de c√≥digo aberto.  Sinta-se livre para usar, modificar e distribuir. 

## ü§ù Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas!  Sinta-se √† vontade para abrir issues ou pull requests.

---

*Feito com üê∏ e muito fine-tuning*
